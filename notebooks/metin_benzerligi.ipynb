{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af338f17-baf7-4271-b3a7-08ebb8d38c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yunus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerekli kütüphaneler ve NLTK verileri yüklendi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yunus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import ast # CSV'den dize literal gösterimlerini güvenli bir şekilde değerlendirmek için\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# NLTK verilerini indirme (henüz indirilmediyse)\n",
    "try:\n",
    "    stopwords.words('turkish') # Türkçe stop kelimeleri kontrol ediyorum\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "print(\"Gerekli kütüphaneler ve NLTK verileri yüklendi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722e04fc-f306-434e-9113-b25e7c334e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\yunus\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\yunus\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\yunus\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yunus\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e3dc62-08a6-49bc-8e02-5d86c5197cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Konfigürasyon tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "# Verilerimin ve modellerimin bulunduğu yolları ayarlıyorum\n",
    "DATA_DIR = \"C:/Users/yunus/Desktop/Bahar Dersleri/Yapay Zeka Proje/arac-arizadeseni-eslestirme/data/processed/\"\n",
    "MODELS_DIR = \"C:/Users/yunus/Desktop/Bahar Dersleri/Yapay Zeka Proje/arac-arizadeseni-eslestirme/models/\"\n",
    "RAW_DATA_DIR = \"C:/Users/yunus/Desktop/Bahar Dersleri/Yapay Zeka Proje/arac-arizadeseni-eslestirme/data/raw/\"\n",
    "\n",
    "\n",
    "LEM_DATA_PATH = DATA_DIR + \"preprocessed_data_lemmatized_only.csv\"\n",
    "STEM_DATA_PATH = DATA_DIR + \"preprocessed_data_stemmed_only.csv\"\n",
    "ALL_RAW_DATA_PATH = RAW_DATA_DIR + \"all_data.csv\" # Orijinal yorumları görüntülemek için\n",
    "\n",
    "\n",
    "# Word2Vec model parametreleri (eğittiğim şekilde eşleşmeli)\n",
    "W2V_PARAMS = [\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "    {'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "    {'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "    {'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]\n",
    "\n",
    "print(\"Konfigürasyon tamamlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c949028-3e6c-4cf4-ac89-e922c2433d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Giriş Metni (Orijinal): doc1 ---\n",
      "'Since 2011, there have been numerous issues. The vehicle has not reached 40,000 miles because there is always something wrong with it. As of now, there is transmission failure, jerking, shuddering, electrical problems, and steering problems. Also, the engine dies while driving. I want to get rid of it, but CarMax told the vehicle is worthless in its condition. This car has been a nightmare.'\n",
      "\n",
      "Veriler yüklendi ve ön işleme fonksiyonları hazır.\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text_lemmatized(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'[^\\\\w\\\\s]', '', sentence)\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        stop_words = set(stopwords.words('english')) # Verilerim İngilizce göründüğü için 'english' kullanıyorum\n",
    "        tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens)) # Belirteçleri TF-IDF için tekrar dizeye birleştiriyorum\n",
    "    return ' '.join(processed_sentences) # TF-IDF için tek bir dize olarak döndürüyorum\n",
    "\n",
    "def preprocess_text_stemmed(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'[^\\\\w\\\\s]', '', sentence)\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        stop_words = set(stopwords.words('english')) # Verilerim İngilizce göründüğü için 'english' kullanıyorum\n",
    "        tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens)) # Belirteçleri TF-IDF için tekrar dizeye birleştiriyorum\n",
    "    return ' '.join(processed_sentences) # TF-IDF için tek bir dize olarak döndürüyorum\n",
    "\n",
    "# Word2Vec için belirteç listesi gerekiyor, tek bir dize değil\n",
    "def preprocess_text_w2v(text, preprocessor_type='lemmatized'):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    all_tokens = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub(r'[^\\\\w\\\\s]', '', sentence)\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "        if preprocessor_type == 'lemmatized':\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        elif preprocessor_type == 'stemmed':\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "        all_tokens.extend(tokens) # Tüm belge için düz bir belirteç listesi elde etmek için append yerine extend kullanıyorum\n",
    "    return all_tokens\n",
    "\n",
    "# Yardımcı fonksiyon: Orijinal yorumları alma\n",
    "def get_original_comment(doc_id):\n",
    "    \"\"\"Bir document_id verildiğinde orijinal yorum metnini alır.\"\"\"\n",
    "    # doc_id 'doc1' gibi, bunu bir tamsayı dizinine dönüştürmem gerekiyor\n",
    "    idx = int(doc_id.replace('doc', '')) - 1\n",
    "    if idx < len(df_raw):\n",
    "        return df_raw.loc[idx, 'comments']\n",
    "    return \"Yorum bulunamadı.\"\n",
    "\n",
    "# Önceden işlenmiş verileri yüklüyorum\n",
    "df_raw = pd.read_csv(ALL_RAW_DATA_PATH)\n",
    "df_raw['document_id'] = ['doc' + str(i + 1) for i in df_raw.index]\n",
    "\n",
    "df_lemmatized_processed = pd.read_csv(LEM_DATA_PATH)\n",
    "df_stemmed_processed = pd.read_csv(STEM_DATA_PATH)\n",
    "\n",
    "# Listelerin dize gösterimlerini gerçek belirteç listelerine geri dönüştürüyorum\n",
    "df_lemmatized_processed['comments_processed'] = df_lemmatized_processed['comments_processed'].apply(ast.literal_eval)\n",
    "df_stemmed_processed['comments_stemmed'] = df_stemmed_processed['comments_stemmed'].apply(ast.literal_eval)\n",
    "\n",
    "# Giriş metnini tanımlıyorum ve ön işleme tabi tutuyorum\n",
    "input_text_id = 'doc1' # PDF yönergelerine göre doc1'i seçtim [cite: 92]\n",
    "input_text_raw = df_raw[df_raw['document_id'] == input_text_id]['comments'].iloc[0]\n",
    "print(f\"--- Giriş Metni (Orijinal): {input_text_id} ---\\n'{input_text_raw}'\\n\")\n",
    "\n",
    "input_text_lemmatized_tfidf = preprocess_text_lemmatized(input_text_raw)\n",
    "input_text_stemmed_tfidf = preprocess_text_stemmed(input_text_raw)\n",
    "input_tokens_lemmatized_w2v = preprocess_text_w2v(input_text_raw, 'lemmatized')\n",
    "input_tokens_stemmed_w2v = preprocess_text_w2v(input_text_raw, 'stemmed')\n",
    "\n",
    "print(\"Veriler yüklendi ve ön işleme fonksiyonları hazır.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7370e9-b566-4e0b-9586-a6404b693263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF Benzerlik Hesaplamaları ---\n",
      "\n",
      "TF-IDF (Lemmatized) Modeli:\n",
      "En Benzer 5 Belge (TF-IDF Lemmatized):\n",
      "  Belge ID: doc2888, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc970, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc968, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc967, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc966, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "\n",
      "TF-IDF (Stemmed) Modeli:\n",
      "En Benzer 5 Belge (TF-IDF Stemmed):\n",
      "  Belge ID: doc429, Skor: 0.3396, Metin: 'Engine burnt oil rather quickly. I have to put in at least 3 Qt. of 5W-30 every week (~ 300 Miles - freeway driving).'\n",
      "  Belge ID: doc971, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc969, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc968, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "  Belge ID: doc967, Skor: 0.0000, Metin: 'Yorum bulunamadı.'\n",
      "TF-IDF benzerlik hesaplamaları tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- TF-IDF Benzerlik Hesaplamaları ---\")\n",
    "\n",
    "# TF-IDF Lemmatized\n",
    "print(\"\\nTF-IDF (Lemmatized) Modeli:\")\n",
    "# Tutarlı bir kelime hazinesi ve IDF sağlamak için TfidfVectorizer'ı tüm metin üzerinde yeniden eğitiyorum.\n",
    "# TF-IDF vektörleştirme için tüm yorumları tek bir dize listesinde birleştiriyorum.\n",
    "corpus_lemmatized_tfidf = [ ' '.join(tokens) for sublist in df_lemmatized_processed['comments_processed'] for tokens in sublist ]\n",
    "tfidf_vectorizer_lemma = TfidfVectorizer()\n",
    "tfidf_matrix_lemma = tfidf_vectorizer_lemma.fit_transform(corpus_lemmatized_tfidf)\n",
    "input_vector_lemma = tfidf_vectorizer_lemma.transform([input_text_lemmatized_tfidf])\n",
    "cosine_similarities_lemma = cosine_similarity(input_vector_lemma, tfidf_matrix_lemma).flatten()\n",
    "\n",
    "# En benzer 5 belgeyi alıyorum\n",
    "top_5_indices_lemma_tfidf = cosine_similarities_lemma.argsort()[-5:][::-1]\n",
    "top_5_docs_lemma_tfidf = []\n",
    "for i in top_5_indices_lemma_tfidf:\n",
    "    doc_id = 'doc' + str(i + 1)\n",
    "    similarity_score = cosine_similarities_lemma[i]\n",
    "    top_5_docs_lemma_tfidf.append({'document_id': doc_id, 'score': similarity_score, 'text': get_original_comment(doc_id)})\n",
    "\n",
    "print(\"En Benzer 5 Belge (TF-IDF Lemmatized):\")\n",
    "for doc in top_5_docs_lemma_tfidf:\n",
    "    print(f\"  Belge ID: {doc['document_id']}, Skor: {doc['score']:.4f}, Metin: '{doc['text']}'\")\n",
    "\n",
    "# TF-IDF Stemmed\n",
    "print(\"\\nTF-IDF (Stemmed) Modeli:\")\n",
    "corpus_stemmed_tfidf = [ ' '.join(tokens) for sublist in df_stemmed_processed['comments_stemmed'] for tokens in sublist ]\n",
    "tfidf_vectorizer_stem = TfidfVectorizer()\n",
    "tfidf_matrix_stem = tfidf_vectorizer_stem.fit_transform(corpus_stemmed_tfidf)\n",
    "input_vector_stem = tfidf_vectorizer_stem.transform([input_text_stemmed_tfidf])\n",
    "cosine_similarities_stem = cosine_similarity(input_vector_stem, tfidf_matrix_stem).flatten()\n",
    "\n",
    "# En benzer 5 belgeyi alıyorum\n",
    "top_5_indices_stem_tfidf = cosine_similarities_stem.argsort()[-5:][::-1]\n",
    "top_5_docs_stem_tfidf = []\n",
    "for i in top_5_indices_stem_tfidf:\n",
    "    doc_id = 'doc' + str(i + 1)\n",
    "    similarity_score = cosine_similarities_stem[i]\n",
    "    top_5_docs_stem_tfidf.append({'document_id': doc_id, 'score': similarity_score, 'text': get_original_comment(doc_id)})\n",
    "\n",
    "print(\"En Benzer 5 Belge (TF-IDF Stemmed):\")\n",
    "for doc in top_5_docs_stem_tfidf:\n",
    "    print(f\"  Belge ID: {doc['document_id']}, Skor: {doc['score']:.4f}, Metin: '{doc['text']}'\")\n",
    "\n",
    "print(\"TF-IDF benzerlik hesaplamaları tamamlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f692af3-93d0-4bb6-b049-ddc9d25aaab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Word2Vec Benzerlik Hesaplamaları ---\n",
      "\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc89, Skor: 0.9911, Metin: 'fix it!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\n",
      "  Belge ID: doc272, Skor: 0.9907, Metin: 'I bought a 2014 Ford Focus used on 10/05/2015. It is the biggest piece of crap I have ever owned. It was a rental car when purchased from Manly Honda in Santa Rosa. I would like to have the fixed it under the California Lemon Law.'\n",
      "  Belge ID: doc361, Skor: 0.9907, Metin: 'I have noticed this problem on similar vehicles. Recalls were made, however I was never notified. It is becoming worse as time goes on.'\n",
      "  Belge ID: doc418, Skor: 0.9907, Metin: '2017 Honda CRV IS SO POOR.'\n",
      "  Belge ID: doc143, Skor: 0.9907, Metin: 'This is the second one on the same vehicle. Makes American made stuff not worth looking at. Thanks Cry Slur! You just worry yourself about making sure you can fart through silk shorts while you put out shoddy cheap breakable product. I was a jeep fan, now you have broken the trust.'\n",
      "\n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc171, Skor: 0.9963, Metin: 'MIL has been off/on for under 3 weeks. I've spent >$600 at 2 separate places w/o a fix. Code initially read, thermostat and misfire in coil #4...had both replaced; still MIL indicates misfire in #4 and multiple misfires..get it fixed. MIL now on indicating bad head. WTF! I heard the 2012 JKUs had a recall on the 3.2L V6 for this. True? If not, any legitimate knowledge on estimated cost to repair?'\n",
      "  Belge ID: doc143, Skor: 0.9963, Metin: 'This is the second one on the same vehicle. Makes American made stuff not worth looking at. Thanks Cry Slur! You just worry yourself about making sure you can fart through silk shorts while you put out shoddy cheap breakable product. I was a jeep fan, now you have broken the trust.'\n",
      "  Belge ID: doc394, Skor: 0.9962, Metin: 'On my way from work I had an incident of unintended acceleration. I was able to stop my car after about 10 seconds (by switching transmission to Neutral). I brought car to Honda next morning and after two weeks received a letter from Honda Canada stating pedal misapplication or interference with pedal application by unanchored aftermarket floor mat, or both.'\n",
      "  Belge ID: doc42, Skor: 0.9962, Metin: 'My 2000 K2500 6.0L 5sp manual truck has made knocking noises from day one, only for several minutes when you first start it. It now as 60k miles and makes the same noise. I have always changed oil at 3000 miles with an extended mileage frame filter or better. I use 15w45 mobile one synthetic oil. I just added a half bottle of Hyper-lube for zinc replacement it quieted down a little bit. It has never let me down. It has been a good truck despite the noise.'\n",
      "  Belge ID: doc54, Skor: 0.9962, Metin: 'I think the motor should be replaced by GM with the amount of complaints about this .'\n",
      "\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc89, Skor: 0.9925, Metin: 'fix it!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\n",
      "  Belge ID: doc418, Skor: 0.9922, Metin: '2017 Honda CRV IS SO POOR.'\n",
      "  Belge ID: doc361, Skor: 0.9922, Metin: 'I have noticed this problem on similar vehicles. Recalls were made, however I was never notified. It is becoming worse as time goes on.'\n",
      "  Belge ID: doc149, Skor: 0.9922, Metin: 'This is the 2nd of 4 issues. Yeah, I've had all 4 window regulators replaced.'\n",
      "  Belge ID: doc148, Skor: 0.9922, Metin: 'This is the 3rd of 4 issues. Yeah, I've had all 4 window regulators replaced.'\n",
      "\n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.9972, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc205, Skor: 0.9966, Metin: 'Sudden loss of power from Alternator, Radio reboot, loss of AC. Vehicle staggered for a moment then continued on battery power. Low speed with each happening = TIPM is getting hot and faulting.'\n",
      "  Belge ID: doc226, Skor: 0.9959, Metin: 'Easily cracking windshields from Toyota are not something I expected from this manufacturer. Very disappointed after reading the class action lawsuit that currently in litigation. Toyota should own up to this!'\n",
      "  Belge ID: doc374, Skor: 0.9959, Metin: 'I'm also experiencing the \"jerk/lurch' at slow speeds.'\n",
      "  Belge ID: doc245, Skor: 0.9958, Metin: 'O-rings leaking oil into cylinders.Thousands of dollars of repairs to fixCheck engine codes persistentCatalytic converter full of burnt oilO2 sensor trips check engine codes as a result of oil through exhaustThereby won't pass inspection without replacement of cat.converter and sensor, not a fix though.'\n",
      "\n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc152, Skor: 0.9962, Metin: 'Bought this Liberty and since it was winter, never thought to roll down the back window. Hmm, it would not go down. Took it all apart, ordered a replacement regulator and installed it. Now I find out that I have 3 more windows that are gonna break soon. Nice going, Chrysler!'\n",
      "  Belge ID: doc386, Skor: 0.9962, Metin: 'Should have done more research before buying. I did buy it used with 8K on it so now I know why it was traded in, not what the salesman told me, buyer beware!'\n",
      "  Belge ID: doc212, Skor: 0.9962, Metin: '9/4/17 while driving home, I noticed I could not control the radio with the steering wheel controls, no big deal.'\n",
      "  Belge ID: doc145, Skor: 0.9962, Metin: 'The regulator is going out again. I took it to Jeep the first two times and got soaked real well 450.00 the first time and right at 400.00 the second time. Going out again. so I thought I would at least bitch about it. On a fixed income and babying it so I don't have to put out another 400.00.'\n",
      "  Belge ID: doc435, Skor: 0.9962, Metin: 'Excessive oil usage. Noticed that oil was disappearing from dip stick but not leaking on drive way or on engine or chassis, nor coming out exhaust pipe'\n",
      "\n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.9988, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc143, Skor: 0.9987, Metin: 'This is the second one on the same vehicle. Makes American made stuff not worth looking at. Thanks Cry Slur! You just worry yourself about making sure you can fart through silk shorts while you put out shoddy cheap breakable product. I was a jeep fan, now you have broken the trust.'\n",
      "  Belge ID: doc226, Skor: 0.9987, Metin: 'Easily cracking windshields from Toyota are not something I expected from this manufacturer. Very disappointed after reading the class action lawsuit that currently in litigation. Toyota should own up to this!'\n",
      "  Belge ID: doc333, Skor: 0.9987, Metin: 'Audi has fault in this type of model. I believe they never pay much attention on engine reliability, always focus on performance, additional features. I will NEVER, EVER, EVER in my life buy this crap, ever.'\n",
      "  Belge ID: doc212, Skor: 0.9987, Metin: '9/4/17 while driving home, I noticed I could not control the radio with the steering wheel controls, no big deal.'\n",
      "\n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc246, Skor: 0.9971, Metin: 'I bought my 2002 passat in 2009. I have had Warranty until 2012. I always got a synthetic oil changes at factory recommended time/miles.'\n",
      "  Belge ID: doc386, Skor: 0.9971, Metin: 'Should have done more research before buying. I did buy it used with 8K on it so now I know why it was traded in, not what the salesman told me, buyer beware!'\n",
      "  Belge ID: doc169, Skor: 0.9971, Metin: 'I was driving on the FDR in NYC during rush hour when my Jeep Wrangler started to shake violently, then it STALLED in the middle of the FDR ! There was someone behind me. Luckily he wasn't distracted and was able to brake in time and get into another lane. I was very lucky - there could have been a major crash. I was also lucky I did not have a heart attack.'\n",
      "  Belge ID: doc152, Skor: 0.9971, Metin: 'Bought this Liberty and since it was winter, never thought to roll down the back window. Hmm, it would not go down. Took it all apart, ordered a replacement regulator and installed it. Now I find out that I have 3 more windows that are gonna break soon. Nice going, Chrysler!'\n",
      "  Belge ID: doc143, Skor: 0.9971, Metin: 'This is the second one on the same vehicle. Makes American made stuff not worth looking at. Thanks Cry Slur! You just worry yourself about making sure you can fart through silk shorts while you put out shoddy cheap breakable product. I was a jeep fan, now you have broken the trust.'\n",
      "\n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.9989, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc226, Skor: 0.9984, Metin: 'Easily cracking windshields from Toyota are not something I expected from this manufacturer. Very disappointed after reading the class action lawsuit that currently in litigation. Toyota should own up to this!'\n",
      "  Belge ID: doc258, Skor: 0.9983, Metin: 'Let's just call a spade a spade - the 2002 Passat is a piece of  !'\n",
      "  Belge ID: doc374, Skor: 0.9982, Metin: 'I'm also experiencing the \"jerk/lurch' at slow speeds.'\n",
      "  Belge ID: doc205, Skor: 0.9982, Metin: 'Sudden loss of power from Alternator, Radio reboot, loss of AC. Vehicle staggered for a moment then continued on battery power. Low speed with each happening = TIPM is getting hot and faulting.'\n",
      "\n",
      "Stemmed Word2Vec (cbow, VS=100, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc217, Skor: 0.1411, Metin: 'No wipers, no fog lights, no radio, no abs, no traction control, no cruise control.'\n",
      "  Belge ID: doc210, Skor: 0.1290, Metin: 'There are multiple issues/problems with the electrical components that from my research online are common enough that they should be fixed under a recall.'\n",
      "  Belge ID: doc150, Skor: 0.1274, Metin: 'This is the 1st for 4 occurrences. Yes, I've had all 4 window regulators replaced.'\n",
      "  Belge ID: doc389, Skor: 0.1266, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc153, Skor: 0.1266, Metin: 'Huge death wobble when hitting potholes on the highway 65mph and up. Vehicle is brand new. This is BS.'\n",
      "\n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc217, Skor: 0.7459, Metin: 'No wipers, no fog lights, no radio, no abs, no traction control, no cruise control.'\n",
      "  Belge ID: doc150, Skor: 0.7441, Metin: 'This is the 1st for 4 occurrences. Yes, I've had all 4 window regulators replaced.'\n",
      "  Belge ID: doc234, Skor: 0.7441, Metin: 'Without any known object striking it my two-month old windshield developed a crack, noted in the morning when I got in my car. I have not been driving on gravel roads.'\n",
      "  Belge ID: doc212, Skor: 0.7439, Metin: '9/4/17 while driving home, I noticed I could not control the radio with the steering wheel controls, no big deal.'\n",
      "  Belge ID: doc210, Skor: 0.7438, Metin: 'There are multiple issues/problems with the electrical components that from my research online are common enough that they should be fixed under a recall.'\n",
      "\n",
      "Stemmed Word2Vec (cbow, VS=100, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc217, Skor: 0.5493, Metin: 'No wipers, no fog lights, no radio, no abs, no traction control, no cruise control.'\n",
      "  Belge ID: doc210, Skor: 0.5455, Metin: 'There are multiple issues/problems with the electrical components that from my research online are common enough that they should be fixed under a recall.'\n",
      "  Belge ID: doc150, Skor: 0.5450, Metin: 'This is the 1st for 4 occurrences. Yes, I've had all 4 window regulators replaced.'\n",
      "  Belge ID: doc153, Skor: 0.5445, Metin: 'Huge death wobble when hitting potholes on the highway 65mph and up. Vehicle is brand new. This is BS.'\n",
      "  Belge ID: doc212, Skor: 0.5444, Metin: '9/4/17 while driving home, I noticed I could not control the radio with the steering wheel controls, no big deal.'\n",
      "\n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc239, Skor: 0.9321, Metin: 'A Small pebble hit my windsheild, you could barely hear it hit and cracked it. Safelight fixed it.'\n",
      "  Belge ID: doc100, Skor: 0.9321, Metin: 'Failure of TIPM. The inner relay for fuel pump works intermittently or not at all. The car will not start. This failure of the TIPM has also resulted in damage to the fuel pump and has rendered it broken.'\n",
      "  Belge ID: doc217, Skor: 0.9320, Metin: 'No wipers, no fog lights, no radio, no abs, no traction control, no cruise control.'\n",
      "  Belge ID: doc234, Skor: 0.9320, Metin: 'Without any known object striking it my two-month old windshield developed a crack, noted in the morning when I got in my car. I have not been driving on gravel roads.'\n",
      "  Belge ID: doc212, Skor: 0.9319, Metin: '9/4/17 while driving home, I noticed I could not control the radio with the steering wheel controls, no big deal.'\n",
      "\n",
      "Stemmed Word2Vec (cbow, VS=300, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.1945, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc374, Skor: 0.1923, Metin: 'I'm also experiencing the \"jerk/lurch' at slow speeds.'\n",
      "  Belge ID: doc258, Skor: 0.1916, Metin: 'Let's just call a spade a spade - the 2002 Passat is a piece of  !'\n",
      "  Belge ID: doc239, Skor: 0.1895, Metin: 'A Small pebble hit my windsheild, you could barely hear it hit and cracked it. Safelight fixed it.'\n",
      "  Belge ID: doc75, Skor: 0.1889, Metin: 'Very disappointing they know what the problem is and won't fix. Hard to take on a fixed income.'\n",
      "\n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.9304, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc75, Skor: 0.9303, Metin: 'Very disappointing they know what the problem is and won't fix. Hard to take on a fixed income.'\n",
      "  Belge ID: doc374, Skor: 0.9303, Metin: 'I'm also experiencing the \"jerk/lurch' at slow speeds.'\n",
      "  Belge ID: doc239, Skor: 0.9302, Metin: 'A Small pebble hit my windsheild, you could barely hear it hit and cracked it. Safelight fixed it.'\n",
      "  Belge ID: doc258, Skor: 0.9302, Metin: 'Let's just call a spade a spade - the 2002 Passat is a piece of  !'\n",
      "\n",
      "Stemmed Word2Vec (cbow, VS=300, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.5761, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc374, Skor: 0.5755, Metin: 'I'm also experiencing the \"jerk/lurch' at slow speeds.'\n",
      "  Belge ID: doc258, Skor: 0.5754, Metin: 'Let's just call a spade a spade - the 2002 Passat is a piece of  !'\n",
      "  Belge ID: doc239, Skor: 0.5749, Metin: 'A Small pebble hit my windsheild, you could barely hear it hit and cracked it. Safelight fixed it.'\n",
      "  Belge ID: doc75, Skor: 0.5748, Metin: 'Very disappointing they know what the problem is and won't fix. Hard to take on a fixed income.'\n",
      "\n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4):\n",
      "En Benzer 5 Belge:\n",
      "  Belge ID: doc389, Skor: 0.9871, Metin: 'Experience increased acceleration when braking nearly hitting a building.'\n",
      "  Belge ID: doc374, Skor: 0.9871, Metin: 'I'm also experiencing the \"jerk/lurch' at slow speeds.'\n",
      "  Belge ID: doc75, Skor: 0.9871, Metin: 'Very disappointing they know what the problem is and won't fix. Hard to take on a fixed income.'\n",
      "  Belge ID: doc146, Skor: 0.9871, Metin: 'Both rear windows and passenger front broke. Cost $225 at mechanic, about $100 to do it yourself. I would suggest to do it yourself as they will break again. 3rd time I have replaced.'\n",
      "  Belge ID: doc239, Skor: 0.9871, Metin: 'A Small pebble hit my windsheild, you could barely hear it hit and cracked it. Safelight fixed it.'\n",
      "Word2Vec benzerlik hesaplamaları tamamlandı.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Word2Vec Benzerlik Hesaplamaları ---\")\n",
    "\n",
    "# Ortalama vektör hesaplama fonksiyonu (Word2Vec için)\n",
    "def get_avg_vector(tokens, model):\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    # Modelde vektör temsili olmayan kelime mevcutsa bunları atlamak aksi taktirde hata mesajı alırım\n",
    "    return np.zeros(model.vector_size) # Modelde kelime bulunamazsa sıfır vektörü döndürüyorum\n",
    "\n",
    "# Jaccard karşılaştırması için tüm sonuçları saklıyorum\n",
    "all_model_top_docs = {}\n",
    "\n",
    "# Lemmatized Word2Vec modelleri için işlem yapıyorum\n",
    "for param in W2V_PARAMS:\n",
    "    model_type = param['model_type']\n",
    "    vector_size = param['vector_size']\n",
    "    window = param['window']\n",
    "    model_filename = f\"lemmatized_model_{model_type}_vs{vector_size}_w{window}.model\"\n",
    "    model_name = f\"Lemmatized Word2Vec ({model_type}, VS={vector_size}, W={window})\"\n",
    "\n",
    "    try:\n",
    "        model = Word2Vec.load(MODELS_DIR + model_filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Uyarı: {model_filename} modeli bulunamadı. Atlanıyor.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "\n",
    "    # Giriş metni için ortalama vektörü alıyorum\n",
    "    input_avg_vec = get_avg_vector(input_tokens_lemmatized_w2v, model)\n",
    "\n",
    "    # Korpustaki tüm belgelerle benzerlikleri hesaplıyorum\n",
    "    document_vectors = []\n",
    "    for doc_token_lists in df_lemmatized_processed['comments_processed']:\n",
    "        flat_doc_tokens = [token for sublist in doc_token_lists for token in sublist]\n",
    "        document_vectors.append(get_avg_vector(flat_doc_tokens, model))\n",
    "\n",
    "    document_vectors_matrix = np.array(document_vectors)\n",
    "\n",
    "    # Giriş vektörü ile tüm belge vektörleri arasındaki kosinüs benzerliğini hesaplıyorum\n",
    "    similarities = cosine_similarity(input_avg_vec.reshape(1, -1), document_vectors_matrix).flatten()\n",
    "\n",
    "    # En benzer 5 belgeyi alıyorum\n",
    "    top_5_indices = similarities.argsort()[-5:][::-1]\n",
    "    top_5_docs = []\n",
    "    for i in top_5_indices:\n",
    "        doc_id = 'doc' + str(i + 1)\n",
    "        similarity_score = similarities[i]\n",
    "        top_5_docs.append({'document_id': doc_id, 'score': similarity_score, 'text': get_original_comment(doc_id)})\n",
    "\n",
    "    print(\"En Benzer 5 Belge:\")\n",
    "    for doc in top_5_docs:\n",
    "        print(f\"  Belge ID: {doc['document_id']}, Skor: {doc['score']:.4f}, Metin: '{doc['text']}'\")\n",
    "\n",
    "    # Jaccard benzerliği için en benzer 5 belge ID'sini saklıyorum\n",
    "    all_model_top_docs[model_name] = [doc['document_id'] for doc in top_5_docs]\n",
    "\n",
    "# Stemmed Word2Vec modelleri için işlem yapıyorum\n",
    "for param in W2V_PARAMS:\n",
    "    model_type = param['model_type']\n",
    "    vector_size = param['vector_size']\n",
    "    window = param['window']\n",
    "    model_filename = f\"stemmed_model_{model_type}_vs{vector_size}_w{window}.model\"\n",
    "    model_name = f\"Stemmed Word2Vec ({model_type}, VS={vector_size}, W={window})\"\n",
    "\n",
    "    try:\n",
    "        model = Word2Vec.load(MODELS_DIR + model_filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Uyarı: {model_filename} modeli bulunamadı. Atlanıyor.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{model_name}:\")\n",
    "\n",
    "    # Giriş metni için ortalama vektörü alıyorum\n",
    "    input_avg_vec = get_avg_vector(input_tokens_stemmed_w2v, model)\n",
    "\n",
    "    # Korpustaki tüm belgelerle benzerlikleri hesaplıyorum\n",
    "    document_vectors = []\n",
    "    for doc_token_lists in df_stemmed_processed['comments_stemmed']:\n",
    "        flat_doc_tokens = [token for sublist in doc_token_lists for token in sublist]\n",
    "        document_vectors.append(get_avg_vector(flat_doc_tokens, model))\n",
    "    document_vectors_matrix = np.array(document_vectors)\n",
    "\n",
    "    similarities = cosine_similarity(input_avg_vec.reshape(1, -1), document_vectors_matrix).flatten()\n",
    "\n",
    "    # En benzer 5 belgeyi alıyorum\n",
    "    top_5_indices = similarities.argsort()[-5:][::-1]\n",
    "    top_5_docs = []\n",
    "    for i in top_5_indices:\n",
    "        doc_id = 'doc' + str(i + 1)\n",
    "        similarity_score = similarities[i]\n",
    "        top_5_docs.append({'document_id': doc_id, 'score': similarity_score, 'text': get_original_comment(doc_id)})\n",
    "\n",
    "    print(\"En Benzer 5 Belge:\")\n",
    "    for doc in top_5_docs:\n",
    "        print(f\"  Belge ID: {doc['document_id']}, Skor: {doc['score']:.4f}, Metin: '{doc['text']}'\")\n",
    "\n",
    "    # Jaccard benzerliği için en benzer 5 belge ID'sini saklıyorum\n",
    "    all_model_top_docs[model_name] = [doc['document_id'] for doc in top_5_docs]\n",
    "\n",
    "# Jaccard hesaplaması için TF-IDF sonuçlarını all_model_top_docs'a ekliyorum\n",
    "all_model_top_docs[\"TF-IDF (Lemmatized)\"] = [doc['document_id'] for doc in top_5_docs_lemma_tfidf]\n",
    "all_model_top_docs[\"TF-IDF (Stemmed)\"] = [doc['document_id'] for doc in top_5_docs_stem_tfidf]\n",
    "\n",
    "print(\"Word2Vec benzerlik hesaplamaları tamamlandı.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3455b833-bab5-41ee-979a-b3edf84d1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Jaccard Benzerlik Matrisi ---\n",
      "                                             Lemmatized Word2Vec (cbow, VS=100, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                         1.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                     0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                         0.43   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                         0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                     0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                         0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                        0.00   \n",
      "TF-IDF (Lemmatized)                                                             0.00   \n",
      "TF-IDF (Stemmed)                                                                0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (skipgram, VS=100, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                             0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                         1.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                         0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                         0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                             0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                         0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                                0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                                0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                                0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                                0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                            0.00   \n",
      "TF-IDF (Lemmatized)                                                                 0.00   \n",
      "TF-IDF (Stemmed)                                                                    0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (cbow, VS=100, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                         0.43   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                         1.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                         0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                         0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                        0.00   \n",
      "TF-IDF (Lemmatized)                                                             0.00   \n",
      "TF-IDF (Stemmed)                                                                0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (skipgram, VS=100, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                         0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                         1.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                         0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                         0.67   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                                0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                                0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                                0.25   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                            0.25   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                                0.25   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                            0.25   \n",
      "TF-IDF (Lemmatized)                                                                 0.00   \n",
      "TF-IDF (Stemmed)                                                                    0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (cbow, VS=300, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                         0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                         0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                         1.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                     0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                         0.25   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                        0.11   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                            0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                        0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                        0.00   \n",
      "TF-IDF (Lemmatized)                                                             0.00   \n",
      "TF-IDF (Stemmed)                                                                0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (skipgram, VS=300, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                             0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                         0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                         0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                             0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                         1.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                             0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                         0.25   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                                0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                            0.11   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                                0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                            0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                                0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                            0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                                0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                            0.11   \n",
      "TF-IDF (Lemmatized)                                                                 0.00   \n",
      "TF-IDF (Stemmed)                                                                    0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (cbow, VS=300, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                         0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                     0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                         0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                     0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                         0.25   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                     0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                         1.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                        0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                        0.00   \n",
      "TF-IDF (Lemmatized)                                                             0.00   \n",
      "TF-IDF (Stemmed)                                                                0.00   \n",
      "\n",
      "                                             Lemmatized Word2Vec (skipgram, VS=300, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                         0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                         0.67   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                         0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                             0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                         1.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                                0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                                0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                            0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                                0.43   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                            0.43   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                                0.43   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                            0.25   \n",
      "TF-IDF (Lemmatized)                                                                 0.00   \n",
      "TF-IDF (Stemmed)                                                                    0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (cbow, VS=100, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                  0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                  0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                  0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                  0.11   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                         1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                     0.43   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                         0.67   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                     0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                         0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                     0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                         0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                     0.11   \n",
      "TF-IDF (Lemmatized)                                                          0.00   \n",
      "TF-IDF (Stemmed)                                                             0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (skipgram, VS=100, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                          0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                      0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                      0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                             0.43   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                         1.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                             0.67   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                         0.43   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                             0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                         0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                             0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                         0.00   \n",
      "TF-IDF (Lemmatized)                                                              0.00   \n",
      "TF-IDF (Stemmed)                                                                 0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (cbow, VS=100, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                  0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                  0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                      0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                  0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                  0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                         0.67   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                     0.67   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                         1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                     0.25   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                         0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                         0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                     0.00   \n",
      "TF-IDF (Lemmatized)                                                          0.00   \n",
      "TF-IDF (Stemmed)                                                             0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (skipgram, VS=100, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                          0.11   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                      0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                      0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                             0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                         0.43   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                             0.25   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                         1.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                             0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                         0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                             0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                         0.11   \n",
      "TF-IDF (Lemmatized)                                                              0.00   \n",
      "TF-IDF (Stemmed)                                                                 0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (cbow, VS=300, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                  0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                  0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                  0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                  0.43   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                         0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                         0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                     0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                         1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                     1.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                         1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                     0.67   \n",
      "TF-IDF (Lemmatized)                                                          0.00   \n",
      "TF-IDF (Stemmed)                                                             0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (skipgram, VS=300, W=2)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                      0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                      0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                      0.43   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                             0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                         0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                             0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                         0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                             1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                         1.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                             1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                         0.67   \n",
      "TF-IDF (Lemmatized)                                                              0.00   \n",
      "TF-IDF (Stemmed)                                                                 0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (cbow, VS=300, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                  0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                  0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                  0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                      0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                  0.43   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                         0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                     0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                         0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                     0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                         1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                     1.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                         1.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                     0.67   \n",
      "TF-IDF (Lemmatized)                                                          0.00   \n",
      "TF-IDF (Stemmed)                                                             0.00   \n",
      "\n",
      "                                             Stemmed Word2Vec (skipgram, VS=300, W=4)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                                      0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                                      0.25   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                                      0.11   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                                          0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                                      0.25   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                                             0.11   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                                         0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                                             0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                                         0.11   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                                             0.67   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                                         0.67   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                                             0.67   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                                         1.00   \n",
      "TF-IDF (Lemmatized)                                                              0.00   \n",
      "TF-IDF (Stemmed)                                                                 0.00   \n",
      "\n",
      "                                             TF-IDF (Lemmatized)  \\\n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                     0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)                 0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                     0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)                 0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                     0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)                 0.00   \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                     0.00   \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)                 0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                        0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                    0.00   \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                        0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                    0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                        0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                    0.00   \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                        0.00   \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                    0.00   \n",
      "TF-IDF (Lemmatized)                                         1.00   \n",
      "TF-IDF (Stemmed)                                            0.25   \n",
      "\n",
      "                                             TF-IDF (Stemmed)  \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=2)                  0.00  \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=2)              0.00  \n",
      "Lemmatized Word2Vec (cbow, VS=100, W=4)                  0.00  \n",
      "Lemmatized Word2Vec (skipgram, VS=100, W=4)              0.00  \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=2)                  0.00  \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=2)              0.00  \n",
      "Lemmatized Word2Vec (cbow, VS=300, W=4)                  0.00  \n",
      "Lemmatized Word2Vec (skipgram, VS=300, W=4)              0.00  \n",
      "Stemmed Word2Vec (cbow, VS=100, W=2)                     0.00  \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=2)                 0.00  \n",
      "Stemmed Word2Vec (cbow, VS=100, W=4)                     0.00  \n",
      "Stemmed Word2Vec (skipgram, VS=100, W=4)                 0.00  \n",
      "Stemmed Word2Vec (cbow, VS=300, W=2)                     0.00  \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=2)                 0.00  \n",
      "Stemmed Word2Vec (cbow, VS=300, W=4)                     0.00  \n",
      "Stemmed Word2Vec (skipgram, VS=300, W=4)                 0.00  \n",
      "TF-IDF (Lemmatized)                                      0.25  \n",
      "TF-IDF (Stemmed)                                         1.00  \n",
      "\n",
      "Jaccard benzerlik matrisi oluşturuldu ve yazdırıldı.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Jaccard Benzerlik Matrisi ---\")\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    intersection = len(s1.intersection(s2))\n",
    "    union = len(s1.union(s2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "model_names = list(all_model_top_docs.keys())\n",
    "jaccard_matrix = pd.DataFrame(0.0, index=model_names, columns=model_names)\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(model_names)):\n",
    "        model_a_name = model_names[i]\n",
    "        model_b_name = model_names[j]\n",
    "        \n",
    "        # Örneğimle eşleşmek için: \"Model Adı\", \"5 Benzer Metin\"\n",
    "        # ve \"Model tfidf_lemmatized ilk 5 sonucu: A={doc1, doc2, doc3, doc4, doc5}\"\n",
    "        # Jaccard hesaplaması için belgelerin yalnızca belge ID'leri olduğundan emin oluyorum.\n",
    "        list_a = all_model_top_docs[model_a_name]\n",
    "        list_b = all_model_top_docs[model_b_name]\n",
    "        \n",
    "        jaccard_matrix.loc[model_a_name, model_b_name] = jaccard_similarity(list_a, list_b)\n",
    "\n",
    "# Jaccard Matrisini yazdırıyorum (daha iyi okunabilirlik için biçimlendirilmiş)\n",
    "print(jaccard_matrix.round(2))\n",
    "\n",
    "print(\"\\nJaccard benzerlik matrisi oluşturuldu ve yazdırıldı.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
